# ScoreFollower


**What is Score Following?**

Given a symbolic representation of a musical score (i.e. position and duration of notes), and a recording of a human playing the same music, the problem of score following is to align the two inputs with each other. If this is done live, it can be coupled with Optical Music Recognition (OMR, like OCR, but more difficult) to perform automatic page turning, and with a phase vocoder to perform automatic musical accompaniment that adjusts tempo in real time to a performer's tempo changes. This technology can be used for pedagogical purposes, as well as in professional contexts (quite a lot of pianists use iPads these days to read music in actual concerts).

**How does it Work?**

Score following has been called a harder version of speech recognition, and the main algorithms use similar techniques. 
Two of the most common approaches use either Dynamic Time Warping or Hidden Markov Models. This project uses the latter the latter because of flexibility, in a score following HMM a hidden state corresponds to a tempo and location in the score and observations correspond to frames of audio from the microphone or input file. 

Version 1 of this program modeled the score using a discrete Semi-Markov model, from which position was then inferred using the Forward algorithm and tempo using a Kalman filter. However, this proved after some experiment to be inadequate, so the current version uses a Particle Filter instead, which allows for a continuous model of score position with more realistic non-linear, non-Gaussian models (for example, a repetitive passage in the score could give rise to multimodal probability distributions of the position variable if the computer were not sure which repetition the player was on).

To evaluate observation probabilities, a method is needed to compare some frame of audio with a state in the score. Peak detection, as used in polyphonic music transcription, can be used for this, but I found it simpler to compare the frequency spectrums of the incoming signal with “templates” modeled from the score for nearby states using the Kullback–Leibler divergence or another similar metric. Templates can either be generated by approximating the fundamental frequencies + harmonics using Gaussians, or by taking the FFT of a MIDI-generated sound which would supposedly yield more realistic overtone series (though not always). The use of a Constant-Q transform is also being experimented with as it (geometrically-distributed frequencies) more closely aligns with the musical scale and thus may produce more robust observations in response to variations in intonation, vibrato, etc--in addition the smaller number of bins increases performance. In the future, machine learning could be applied to the generated templates, refining them over iterations to fit the incoming audio as the computer “learns” the timbre of the instrument it’s listening to and the habits of the performer regarding voicing chords. 

The other piece of data used is onset detection, which locates note changes/transients in the input signal through a modified version of Spectral Flux. I have found papers using neural networks to do this, but I haven’t had the time to implement one and I suspect the gains will be minimal compared to the performance hit. There may be some way to use neural networks to help with the observation portion of the algorithm, which I haven’t looked into yet.

**What is the state of this project?**

In general, progress is slow at the moment because of schoolwork. The current build, though structured as an ios app, sill lacks a GUI and is mostly used to test/fine tune the algorithms off of a specific dataset. Unfortunately, I have had to exclude this data from the git repository due to GitHub's filesize limits. 

The main challenge going forward will be optimizing the observation process for both performance and accuracy. I’ve managed to avoid setting up a more comprehensive testing framework for some time, partly due to the difficulty of finding symbolic scores and hand-aligned data for specific recordings, but I will probably have to do so soon (too many messy excel datasheets in my documents folder!). The other problem is figuring out how to do OMR on scanned sheet music, which is a whole other field to explore, but I’ve been hoping to use for the promising open-source project Audiveris that is releasing v5 sometime in the next few months after a two-year wait. 
